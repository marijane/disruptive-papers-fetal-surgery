{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Disruptive Papers in Fetal Surgery"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Description\n",
                "\n",
                "This notebook takes as input one or more PubMed CSV exports, placed in the [data/pubmed/results/](data/pubmed/results/) directory. It adds useful columns to the CSV file, maps the PubMed IDs to Microsoft Academic Graph IDs, uses the MAGIDs to look up citation and disruption data from Wu et al., 2023 to score each article, and saves the scored output to a styled Excel spreadsheet.\n",
                "\n",
                "### PubMed Search Documentation and Replication\n",
                "\n",
                "The searches used to create the PubMed CSV exports are documented in [DisruptivePapersFetalSurgery-PubMedSearchDocumentation-202303.xlsx](data/pubmed/searches/DisruptivePapersFetalSurgery-PubMedSearchDocumentation-202303.xlsx). They cover Congenital Diaphragmatic Hernia (CDH), Congenital Pulmonary Airway Malfunction (CPAM), Neural Tube Defects (NTD), and Twin-to-Twin Transfusion Syndrome (TTTS), with each search documented on a separate spreadsheet tab. \n",
                "\n",
                "The searches can be replicated by either copying and pasting the full search string from each spreadsheet tab into PubMed or clicking on the number in the Results column. Once replicated, the search results can be exported to CSV via the *Save* button. Select *All results* and *CSV* format, then click *Create file*.\n",
                "\n",
                "### References\n",
                "\n",
                "Wu, L., Wang, D., & Evans, J. (2023). Replication Data for: Large teams develop and small teams disrupt science and technology [Data set]. Harvard Dataverse. https://doi.org/10.7910/DVN/JPWNNK\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Environment"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Import libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import Counter\n",
                "import glob\n",
                "import logging\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import requests\n",
                "from styleframe import StyleFrame, Styler, utils\n",
                "import time\n",
                "from tqdm.notebook import tqdm"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Define constants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "email = \"whimar@ohsu.edu\"\n",
                "openalex = \"https://api.openalex.org/works?per-page=100&filter=pmid:\"\n",
                "headers = {\"user-agent\": \"mailto:\" + email}\n",
                "\n",
                "datestamp = time.strftime(\"%Y%m%d\")\n",
                "TOP_PAPERS = 100\n",
                "\n",
                "DATA_DIR = \"data/\"\n",
                "MAGDISRUPT_DIR = DATA_DIR + \"MAG-disruption/\"\n",
                "# MAGDISRUPT_FILE = MAGDISRUPT_DIR + \"AggregatedMAG.txt\"\n",
                "MAGDISRUPT_FILE = MAGDISRUPT_DIR + \"Aggregated_20210521.txt\"\n",
                "PUBMED_DIR = DATA_DIR + \"pubmed/\"\n",
                "RESULTS_DIR = PUBMED_DIR + \"results/*\"\n",
                "\n",
                "LOG_DIR = \"log/\"\n",
                "\n",
                "TARGET_DIR = \"target/\"\n",
                "CSV_DIR = TARGET_DIR + \"csv/\"\n",
                "XLSX_DIR = TARGET_DIR + \"xlsx/\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create target directories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Path(LOG_DIR).mkdir(parents=True, exist_ok=True)\n",
                "Path(TARGET_DIR).mkdir(parents=True, exist_ok=True)\n",
                "Path(CSV_DIR).mkdir(parents=True, exist_ok=True)\n",
                "Path(XLSX_DIR).mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Configure logging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logging.basicConfig(\n",
                "    filename=f\"{LOG_DIR}fsdp-{datestamp}.log\",\n",
                "    filemode=\"w\",\n",
                "    force=True,\n",
                "    format=\"%(asctime)s.%(msecs)03d : %(levelname)s : %(message)s\",\n",
                "    level=logging.INFO,\n",
                "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
                ")\n",
                "logger = logging.getLogger()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load disruption data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logger.info(\"Reading disruption data from %s\", MAGDISRUPT_FILE)\n",
                "# AggregatedMAG\n",
                "# MAGDATA_DF = pd.read_csv(\n",
                "#     MAGDISRUPT_FILE,\n",
                "#     sep=\"\\t\",\n",
                "#     usecols=[0, 5, 6],\n",
                "#     names=[\"magid\", \"num_citations\", \"disruption_score\"],\n",
                "#     index_col=\"magid\",\n",
                "#     dtype={\"num_citations\": \"int64\", \"disruption_score\": \"float64\"},\n",
                "# )\n",
                "\n",
                "# Aggregated_20210521\n",
                "MAGDATA_DF = pd.read_csv(\n",
                "    MAGDISRUPT_FILE,\n",
                "    sep=\"\\t\",\n",
                "    usecols=[0, 4, 5],\n",
                "    names=[\"magid\", \"num_citations\", \"disruption_score\"],\n",
                "    index_col=\"magid\",\n",
                "    dtype={\"num_citations\": \"int64\", \"disruption_score\": \"float64\"},\n",
                ")\n",
                "MAGDATA_DF.sort_index(inplace=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Function definitions"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### pmid2magid\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "#### Description\n",
                "\n",
                "Maps PubMed IDs (PMID) from a PubMed CSV export to Microsoft Academic Graph IDs (MAGID) using the [OpenAlex Works API](https://docs.openalex.org/api-entities/works). Caches API results to CSV and reads from CSV if it already exists to reduce API calls.\n",
                "\n",
                "#### Parameters\n",
                "\n",
                "| Parameter | Description |\n",
                "| --- | --- |\n",
                "| articles_df | A processed/augmented DataFrame created from a PubMed CSV export |\n",
                "| topicdate | The stem of the PubMed CSV filename, expected filename convenion *topic-date.csv* |\n",
                "\n",
                "#### Return Values\n",
                "\n",
                "| Return Value | Description |\n",
                "| --- | --- |\n",
                "| mags_df | a two-column dataframe of PMIDs and MAGIDs |\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pmid2magid(articles_df, topicdate):\n",
                "    magsfile = f\"{CSV_DIR}{topicdate}-PMID2MAGID.csv\"\n",
                "    magsfilepath = Path(magsfile)\n",
                "\n",
                "    if magsfilepath.is_file():\n",
                "        logger.info(\"%s : Reading cached MAGID data from file %s\", topicdate, magsfile)\n",
                "        mags_df = pd.read_csv(magsfile, index_col=\"long_pmid\")\n",
                "    else:\n",
                "        logger.info(\"%s : Pulling new MAGID data from OpenAlex\", topicdate)\n",
                "        pmids = articles_df[\"pmid\"].tolist()\n",
                "\n",
                "        # combine PMIDs into groups of 50 to reduce OpenAlex API calls\n",
                "        pmid_groups = [pmids[i : i + 50] for i in range(0, len(pmids), 50)]\n",
                "\n",
                "        pmid_strings = []\n",
                "        mags = {}\n",
                "        for pmid_group in pmid_groups:\n",
                "            pmid_strings.append(\"|\".join(map(str, pmid_group)))\n",
                "\n",
                "        for pmid_string in tqdm(pmid_strings, desc=topicdate + \" openalex\"):\n",
                "            logger.info(\"%s : Requesting PMIDs from OpenAlex: %s\", topicdate, pmid_string)\n",
                "            response = requests.get(openalex + pmid_string, headers=headers)\n",
                "            logger.info(\"%s : OpenAlex response code: %s\", topicdate, response.status_code)\n",
                "            if response.status_code == 200:\n",
                "                works = response.json()\n",
                "                for work in works[\"results\"]:\n",
                "                    if \"mag\" in work[\"ids\"]:\n",
                "                        logger.info(\n",
                "                            \"%s : Found MAGID %s for PMID %s\", topicdate, work[\"ids\"][\"mag\"], work[\"ids\"][\"pmid\"]\n",
                "                        )\n",
                "                        mags[work[\"ids\"][\"pmid\"]] = work[\"ids\"][\"mag\"]\n",
                "                    else:\n",
                "                        logger.info(\"%s : No MAGID found for PMID %s\", topicdate, work[\"ids\"][\"pmid\"])\n",
                "            else:\n",
                "                logger.error(\"%s : Error making OpenAlex request for %s\", topicdate, pmid_string)\n",
                "\n",
                "        mags_df = pd.DataFrame.from_dict(mags, orient=\"index\", columns=[\"magid\"])\n",
                "        mags_df.index.name = \"long_pmid\"\n",
                "        mags_df.sort_index(inplace=True)\n",
                "        num_magid = len(mags_df.index)\n",
                "        num_pmid = len(articles_df.index)\n",
                "        logger.info(\"%s : Found %d MAGIDs from %d PMIDs\", topicdate, num_magid, num_pmid)\n",
                "        logger.info(\"%s : %d MAGIDs not found\", topicdate, num_pmid - num_magid)\n",
                "        logger.info(\"%s : Saving mapped PMID/MAGIDs to %s\", topicdate, magsfile)\n",
                "        mags_df.to_csv(magsfile)\n",
                "        mags_df = pd.read_csv(magsfile, index_col=\"long_pmid\")\n",
                "\n",
                "    return mags_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### score_results\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Description\n",
                "\n",
                "Maps PubMed IDs (PMID) from a PubMed CSV export to citation and disruption score data.\n",
                "\n",
                "#### Parameters\n",
                "\n",
                "| Parameter | Description |\n",
                "| --- | --- |\n",
                "| articles_df | A processed/augmented DataFrame created from a PubMed CSV export |\n",
                "| topicdate | The stem of the PubMed CSV filename, expected format topic-date.csv |\n",
                "\n",
                "#### Return Values\n",
                "\n",
                "| Return Value | Description |\n",
                "| --- | --- |\n",
                "| cited_df | A processed/augmented DataFrame created from a PubMed CSV export and sorted by descending number of citations |\n",
                "| develop_df | A processed/augmented DataFrame created from a PubMed CSV export and sorted from most developmental to least developmental |\n",
                "| disrupt_df |A processed/augmented DataFrame created from a PubMed CSV export and sorted from most disruptive to least disruptive |\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def score_results(articles_df, topicdate):\n",
                "    citefile = f\"{CSV_DIR}{topicdate}-topcited.csv\"\n",
                "    developfile = f\"{CSV_DIR}{topicdate}-topdevelopmental.csv\"\n",
                "    disruptfile = f\"{CSV_DIR}{topicdate}-topdisruptive.csv\"\n",
                "\n",
                "    logger.info(\"%s : Joining article data with disruption data\", topicdate)\n",
                "    scored_df = articles_df.join(MAGDATA_DF, sort=True)\n",
                "    scored_df.reset_index(inplace=True)\n",
                "\n",
                "    logger.info(\"%s : Sorting articles by descending citation count\", topicdate)\n",
                "    cited_df = scored_df[\n",
                "        [\n",
                "            \"num_citations\",\n",
                "            \"title\",\n",
                "            \"journal\",\n",
                "            \"pubdate\",\n",
                "            \"magid\",\n",
                "            \"pmid\",\n",
                "            \"doi\",\n",
                "            \"pubmed\",\n",
                "            \"ohsu_library\",\n",
                "            \"rush_library\",\n",
                "        ]\n",
                "    ].nlargest(TOP_PAPERS, columns=\"num_citations\")\n",
                "\n",
                "    logger.info(\"%s : Saving citation count data to %s\", topicdate, citefile)\n",
                "    cited_df.to_csv(\n",
                "        citefile,\n",
                "        index=False,\n",
                "        columns=[\n",
                "            \"num_citations\",\n",
                "            \"magid\",\n",
                "            \"pmid\",\n",
                "            \"title\",\n",
                "            \"journal\",\n",
                "            \"pubdate\",\n",
                "            \"doi\",\n",
                "        ],\n",
                "    )\n",
                "\n",
                "    logger.info(\"%s : Selecting most developmental articles\", topicdate)\n",
                "    development_df = scored_df[\n",
                "        [\n",
                "            \"disruption_score\",\n",
                "            \"title\",\n",
                "            \"journal\",\n",
                "            \"pubdate\",\n",
                "            \"magid\",\n",
                "            \"pmid\",\n",
                "            \"doi\",\n",
                "            \"pubmed\",\n",
                "            \"ohsu_library\",\n",
                "            \"rush_library\",\n",
                "        ]\n",
                "    ].nsmallest(TOP_PAPERS, columns=\"disruption_score\")\n",
                "\n",
                "    logger.info(\"%s : Saving developmental articles data to %s\", topicdate, developfile)\n",
                "    development_df.to_csv(\n",
                "        developfile,\n",
                "        index=False,\n",
                "        columns=[\n",
                "            \"disruption_score\",\n",
                "            \"magid\",\n",
                "            \"pmid\",\n",
                "            \"title\",\n",
                "            \"journal\",\n",
                "            \"pubdate\",\n",
                "            \"doi\",\n",
                "        ],\n",
                "    )\n",
                "\n",
                "    logger.info(\"%s : Selecting most disruptive articles\", topicdate)\n",
                "    disrupt_sf = scored_df[\n",
                "        [\n",
                "            \"disruption_score\",\n",
                "            \"title\",\n",
                "            \"journal\",\n",
                "            \"pubdate\",\n",
                "            \"magid\",\n",
                "            \"pmid\",\n",
                "            \"doi\",\n",
                "            \"pubmed\",\n",
                "            \"ohsu_library\",\n",
                "            \"rush_library\",\n",
                "        ]\n",
                "    ].nlargest(TOP_PAPERS, columns=\"disruption_score\")\n",
                "\n",
                "    logger.info(\"%s : Saving disruptive articles to %s\", topicdate, disruptfile)\n",
                "    disrupt_sf.to_csv(\n",
                "        disruptfile,\n",
                "        index=False,\n",
                "        columns=[\n",
                "            \"disruption_score\",\n",
                "            \"magid\",\n",
                "            \"pmid\",\n",
                "            \"title\",\n",
                "            \"journal\",\n",
                "            \"pubdate\",\n",
                "            \"doi\",\n",
                "        ],\n",
                "    )\n",
                "\n",
                "    return cited_df, development_df, disrupt_sf"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### style_output\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Description\n",
                "\n",
                "Exports sorted scored dataframes to styled XLSX file.\n",
                "\n",
                "#### Parameters\n",
                "\n",
                "| Parameter | Description |\n",
                "| --- | --- |\n",
                "| cited_df | A processed/augmented DataFrame created from a PubMed CSV export and sorted by descending number of citations |\n",
                "| develop_df | A processed/augmented DataFrame created from a PubMed CSV export and sorted from most developmental to least developmental |\n",
                "| disrupt_df |A processed/augmented DataFrame created from a PubMed CSV export and sorted from most disruptive to least disruptive |\n",
                "| topicdate | The stem of the PubMed CSV filename, expected format topic-date.csv |\n",
                "\n",
                "#### Return Values\n",
                "\n",
                "None."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def style_output(cited_df, develop_df, disrupt_sf, topicdate):\n",
                "    scorefile = f\"{XLSX_DIR}{topicdate}-scores.xlsx\"\n",
                "\n",
                "    cited_sf = StyleFrame(cited_df.drop(\"pmid\", axis=1))\n",
                "    develop_sf = StyleFrame(develop_df.drop(\"pmid\", axis=1))\n",
                "    disrupt_sf = StyleFrame(disrupt_sf.drop(\"pmid\", axis=1))\n",
                "    # query_s = pd.Series({f\"{querytype} query\": query})\n",
                "\n",
                "    default_style = Styler(\n",
                "        font=utils.fonts.calibri,\n",
                "        font_size=11,\n",
                "        border_type=utils.borders.default_grid,\n",
                "        horizontal_alignment=utils.horizontal_alignments.left,\n",
                "        wrap_text=False,\n",
                "        shrink_to_fit=False,\n",
                "    )\n",
                "\n",
                "    header_style = Styler(\n",
                "        bg_color=utils.colors.black,\n",
                "        bold=True,\n",
                "        font=utils.fonts.calibri,\n",
                "        font_color=utils.colors.white,\n",
                "        font_size=14,\n",
                "        horizontal_alignment=utils.horizontal_alignments.left,\n",
                "        shrink_to_fit=False,\n",
                "        wrap_text=False,\n",
                "        vertical_alignment=utils.vertical_alignments.center,\n",
                "    )\n",
                "    hyperlink_style = Styler(\n",
                "        font_color=utils.colors.blue,\n",
                "        protection=True,\n",
                "        underline=utils.underline.single,\n",
                "    )\n",
                "    float_style = Styler(\n",
                "        number_format=\"0.000000000000\",\n",
                "        horizontal_alignment=utils.horizontal_alignments.right,\n",
                "    )\n",
                "\n",
                "    cited_sf.set_column_width_dict(\n",
                "        col_width_dict={\n",
                "            (\"pubdate\", \"magid\", \"pubmed\"): 12,\n",
                "            (\"num_citations\", \"ohsu_library\", \"rush_library\"): 15,\n",
                "            (\"journal\"): 30,\n",
                "            (\"title\", \"doi\"): 50,\n",
                "        }\n",
                "    )\n",
                "    cited_sf.apply_headers_style(header_style)\n",
                "    cited_sf.apply_column_style(cited_sf.columns, styler_obj=default_style)\n",
                "    cited_sf.apply_column_style(\n",
                "        [\"doi\", \"pubmed\", \"ohsu_library\", \"rush_library\"],\n",
                "        styler_obj=Styler.combine(default_style, hyperlink_style),\n",
                "    )\n",
                "\n",
                "    develop_sf.set_column_width_dict(\n",
                "        col_width_dict={\n",
                "            (\"pubdate\", \"magid\", \"pubmed\"): 12,\n",
                "            (\"ohsu_library\", \"rush_library\"): 15,\n",
                "            (\"disruption_score\"): 20,\n",
                "            (\"journal\"): 30,\n",
                "            (\"title\", \"doi\"): 50,\n",
                "        }\n",
                "    )\n",
                "    develop_sf.apply_headers_style(header_style)\n",
                "    develop_sf.apply_column_style(develop_sf.columns, styler_obj=default_style)\n",
                "    develop_sf.apply_column_style(\"disruption_score\", styler_obj=Styler.combine(default_style, float_style))\n",
                "    develop_sf.apply_column_style(\n",
                "        [\"doi\", \"pubmed\", \"ohsu_library\", \"rush_library\"],\n",
                "        styler_obj=Styler.combine(default_style, hyperlink_style),\n",
                "    )\n",
                "\n",
                "    disrupt_sf.set_column_width_dict(\n",
                "        col_width_dict={\n",
                "            (\"pubdate\", \"magid\", \"pubmed\"): 12,\n",
                "            (\"ohsu_library\", \"rush_library\"): 15,\n",
                "            (\"disruption_score\"): 20,\n",
                "            (\"journal\"): 30,\n",
                "            (\"title\", \"doi\"): 50,\n",
                "        }\n",
                "    )\n",
                "    disrupt_sf.apply_headers_style(header_style)\n",
                "    disrupt_sf.apply_column_style(disrupt_sf.columns, styler_obj=default_style)\n",
                "    disrupt_sf.apply_column_style(\"disruption_score\", styler_obj=Styler.combine(default_style, float_style))\n",
                "    disrupt_sf.apply_column_style(\n",
                "        [\"doi\", \"pubmed\", \"ohsu_library\", \"rush_library\"],\n",
                "        styler_obj=Styler.combine(default_style, hyperlink_style),\n",
                "    )\n",
                "\n",
                "    with pd.ExcelWriter(scorefile) as sfile:\n",
                "        logger.info(\"%s : Saving styled citation count data to %s\", topicdate, scorefile)\n",
                "        cited_sf.to_excel(\n",
                "            sfile,\n",
                "            index=False,\n",
                "            columns=[\n",
                "                \"num_citations\",\n",
                "                \"title\",\n",
                "                \"journal\",\n",
                "                \"pubdate\",\n",
                "                \"magid\",\n",
                "                \"doi\",\n",
                "                \"pubmed\",\n",
                "                \"ohsu_library\",\n",
                "                \"rush_library\",\n",
                "            ],\n",
                "            sheet_name=f\"top {TOP_PAPERS} cited\",\n",
                "        )\n",
                "        logger.info(\"%s : Saving styled developmental score data to %s\", topicdate, scorefile)\n",
                "        develop_sf.to_excel(\n",
                "            sfile,\n",
                "            index=False,\n",
                "            columns=[\n",
                "                \"disruption_score\",\n",
                "                \"title\",\n",
                "                \"journal\",\n",
                "                \"pubdate\",\n",
                "                \"magid\",\n",
                "                \"doi\",\n",
                "                \"pubmed\",\n",
                "                \"ohsu_library\",\n",
                "                \"rush_library\",\n",
                "            ],\n",
                "            sheet_name=f\"top {TOP_PAPERS} developmental\",\n",
                "        )\n",
                "\n",
                "        logger.info(\"%s : Saving styled disruption score data to %s\", topicdate, scorefile)\n",
                "        disrupt_sf.to_excel(\n",
                "            sfile,\n",
                "            index=False,\n",
                "            columns=[\n",
                "                \"disruption_score\",\n",
                "                \"title\",\n",
                "                \"journal\",\n",
                "                \"pubdate\",\n",
                "                \"magid\",\n",
                "                \"doi\",\n",
                "                \"pubmed\",\n",
                "                \"ohsu_library\",\n",
                "                \"rush_library\",\n",
                "            ],\n",
                "            sheet_name=f\"top {TOP_PAPERS} disruptive\",\n",
                "        )\n",
                "        # query_s.to_excel(sfile, index=False, header=False, sheet_name=\"pubmed query\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### count_journals\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Description\n",
                "\n",
                "Creates a pivot table of journal name counts.\n",
                "\n",
                "#### Parameters\n",
                "\n",
                "| Parameter | Description |\n",
                "| --- | --- |\n",
                "| csv_list          | A list containing paths to CSV files |\n",
                "| column_numbers    | An optional list of column numbers to read from the file |\n",
                "| header_names      | An optional list of header names to read from the file|\n",
                "\n",
                "#### Return Values\n",
                "\n",
                "A dataframe with the columns \"Journal\" (the name of the journal), \"Count\" (the number of times it appears in the CSVs), and \"Search Source\" (the name of the CSV files the journal name was found in.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_journals(csv_list, column_numbers=None, header_names=None):\n",
                "    # Initialize a Counter to store occurrences\n",
                "    journal_counter = Counter()\n",
                "\n",
                "    # Initialize a dictionary to store aggregated search sources\n",
                "    search_source_aggregated = {}\n",
                "\n",
                "    # Iterate through each CSV file in the list\n",
                "    for CSV in csv_list:\n",
                "        topicdate = Path(CSV).stem\n",
                "\n",
                "        logger.info(\"%s : Reading search results from  CSV %s\", topicdate, CSV)\n",
                "\n",
                "        # Read the CSV file\n",
                "        pmresults_df = pd.read_csv(\n",
                "            CSV,\n",
                "            usecols=column_numbers,\n",
                "            names=header_names,\n",
                "        )\n",
                "        # Add a column \"search_source\" containing the value of CSV\n",
                "        pmresults_df[\"search_source\"] = topicdate\n",
                "\n",
                "        # Update the counter with the values from the \"journal\" column\n",
                "        journal_counter.update(pmresults_df[\"journal\"].dropna())\n",
                "\n",
                "        # Aggregate search_source for each journal\n",
                "        for journal, sources in pmresults_df.groupby(\"journal\")[\"search_source\"]:\n",
                "            if journal not in search_source_aggregated:\n",
                "                search_source_aggregated[journal] = set()\n",
                "            search_source_aggregated[journal].update(sources)\n",
                "\n",
                "    # Sort the counter by occurrences in descending order\n",
                "    sortedjournal_counts = journal_counter.most_common()\n",
                "\n",
                "    # Prepare the final dataframe\n",
                "    journal_counts_data = []\n",
                "    for journal, count in sortedjournal_counts:\n",
                "        aggregated_sources = \"|\".join(sorted(search_source_aggregated[journal]))\n",
                "        journal_counts_data.append((journal, count, aggregated_sources))\n",
                "\n",
                "    journal_counts_df = pd.DataFrame(journal_counts_data, columns=[\"Journal\", \"Count\", \"Search_Source\"])\n",
                "\n",
                "    return journal_counts_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Read and Augment PubMed CSV for MAGID mapping and citation/disruption scoring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pubmedCSVs = sorted(glob.glob(RESULTS_DIR))\n",
                "\n",
                "for pubmedCSV in pubmedCSVs:\n",
                "    topicdate = Path(pubmedCSV).stem\n",
                "\n",
                "    logger.info(\"%s : Reading search results from PubMed CSV %s\", topicdate, pubmedCSV)\n",
                "\n",
                "    pmresults_df = pd.read_csv(\n",
                "        pubmedCSV, header=0, usecols=[0, 1, 5, 6, 10], names=[\"pmid\", \"title\", \"journal\", \"pubdate\", \"doi\"]\n",
                "    )\n",
                "\n",
                "    # add columns of helpful URLs\n",
                "    # hyperlinked URLs makes publications more accessible in the XLSX output\n",
                "\n",
                "    # DOI link, blank if no DOI\n",
                "    pmresults_df[\"doi\"] = [\n",
                "        f'=HYPERLINK(\"https://doi.org/{doi}\")' if pd.notna(doi) else \"\" for doi in pmresults_df[\"doi\"]\n",
                "    ]\n",
                "\n",
                "    # Link to PubMed record\n",
                "    pmresults_df[\"pubmed\"] = [\n",
                "        f'=HYPERLINK(\"https://pubmed.ncbi.nlm.nih.gov/{pmid}\", {pmid})' for pmid in pmresults_df[\"pmid\"]\n",
                "    ]\n",
                "\n",
                "    # Link to OHSU Library Catalog lookup for full-text access\n",
                "    pmresults_df[\"ohsu_library\"] = [\n",
                "        f'=HYPERLINK(\"https://librarysearch.ohsu.edu/openurl/OHSU/OHSU?sid=Entrez:PubMed&id=pmid:{pmid}\", \"Find @ OHSU\")'\n",
                "        for pmid in pmresults_df[\"pmid\"]\n",
                "    ]\n",
                "\n",
                "    # Link to Rush University Library Catalog lookup for full-text access\n",
                "    pmresults_df[\"rush_library\"] = [\n",
                "        f'=HYPERLINK(\"https://i-share-rsh.primo.exlibrisgroup.com/openurl/01CARLI_RSH/01CARLI_RSH:CARLI_RSH?sid=Entrez:PubMed&id=pmid:{pmid}\", \"Find @ Rush\")'\n",
                "        for pmid in pmresults_df[\"pmid\"]\n",
                "    ]\n",
                "\n",
                "    # \"Long PMID\" for MAGID mapping in OpenAlex\n",
                "    pmresults_df[\"long_pmid\"] = [f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}\" for pmid in pmresults_df[\"pmid\"]]\n",
                "\n",
                "    pmresults_df.set_index(\"long_pmid\", inplace=True)\n",
                "    pmresults_df.sort_index(inplace=True)\n",
                "    pmresults_df.to_csv(f\"{CSV_DIR}{topicdate}-longPMID.csv\")\n",
                "    pmresults_df = pd.read_csv(f\"{CSV_DIR}{topicdate}-longPMID.csv\", index_col=\"long_pmid\")\n",
                "\n",
                "    logger.info(\"%s : Mapping PMIDs to MAGIDs\", topicdate)\n",
                "    mags_df = pmid2magid(pmresults_df, topicdate)\n",
                "\n",
                "    logger.info(\"%s : Joining PubMed search results with MAGID mapping\", topicdate)\n",
                "    articles_df = mags_df.join(pmresults_df).set_index(\"magid\").sort_index()\n",
                "    articles_df.index = articles_df.index.astype(\"int64\")\n",
                "    articles_df.to_csv(f\"{CSV_DIR}{topicdate}-MAGID.csv\")\n",
                "    articles_df = pd.read_csv(f\"{CSV_DIR}{topicdate}-MAGID.csv\", index_col=\"magid\")\n",
                "\n",
                "    logger.info(\"%s : Scoring articles\", topicdate)\n",
                "    articles_cited_df, articles_develop_df, articles_disrupt_df = score_results(articles_df, topicdate)\n",
                "\n",
                "    logger.info(\"%s : Styling output XSLX\", topicdate)\n",
                "    style_output(articles_cited_df, articles_develop_df, articles_disrupt_df, topicdate)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Count Journals"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pubmed_journal_counts_df = count_journals(pubmedCSVs, [0, 1, 5, 6, 10], [\"pmid\", \"title\", \"journal\", \"pubdate\", \"doi\"])\n",
                "pubmed_journal_counts_df.to_csv(f\"{CSV_DIR}search_journal_counts.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scoredCSVs = sorted(glob.glob(f\"{CSV_DIR}*top*.csv\"))\n",
                "scored_journal_counts_df = count_journals(scoredCSVs)\n",
                "scored_journal_counts_df.to_csv(f\"{CSV_DIR}scored_journal_counts.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scored_journal_counts_df"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
